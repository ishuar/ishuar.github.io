{"status":"ok","feed":{"url":"https://medium.com/feed/@ishuar","title":"Stories by Ishan Sharma on Medium","link":"https://medium.com/@ishuar?source=rss-6962bfed9f3------2","author":"","description":"Stories by Ishan Sharma on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*0-drjvfIALjYoFRAS60NYw.png"},"items":[{"title":"Expanding Persistent Volumes for Statefulsets in Kubernetes","pubDate":"2023-11-23 12:56:12","link":"https://ishuar.medium.com/expanding-persistent-volumes-for-statefulsets-in-kubernetes-ecd7cf75b754?source=rss-6962bfed9f3------2","guid":"https://medium.com/p/ecd7cf75b754","author":"Ishan Sharma","thumbnail":"https://cdn-images-1.medium.com/max/1024/0*zTlqffP7_Fd9nHPJ","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*zTlqffP7_Fd9nHPJ\"><figcaption>Photo by <a href=\"https://unsplash.com/@growtika?utm_source=medium&amp;utm_medium=referral\">Growtika</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3><strong>Expanding Persistent Volumes for StatefulSets in Kubernetes</strong></h3>\n<p>Expanding volumes in Kubernetes clusters is a routine maintenance activity and seemingly straightforward\u200a\u2014\u200aor so I thought! Recently, a unique challenge arose when there was a request to increase the storage size for a Stateful Application running in a Kubernetes cluster.</p>\n<p>Let\u2019s jump into the story to share the real-world experience of this\u00a0request.</p>\n<h4><strong>Problem Statement</strong></h4>\n<p>In the realm of Kubernetes clusters, a statefulset application thrived. Its popularity soared to the extent that it demanded more storage than the currently attached volume could provide. The team, recognizing the need to meet this demand, set out to expand the storage of the persistent volume connected to the statefulset.<em> However, quickly realized that this was easier said than\u00a0done.</em></p>\n<p>As<a href=\"https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/\"> Kubernetes 1.24</a> supports volume expansion, Just like anyone who had the first encounter with this task, I anticipated that it was as simple as increasing the spec.volumeClaimTemplates.spec.resources.requests.storage in the statefulset application. <strong>**But**</strong> It was not\u00a0!!</p>\n<p>Contrary to initial expectations, modifying the spec.volumeClaimTemplates.spec.resources.requests.storage alone does not trigger the expansion of volumes attached to statefulsets. Manual intervention is necessary to navigate this intricate landscape. Let\u2019s learn how can we achieve it in this\u00a0article.</p>\n<h4><strong>Prerequisites For Persistent Volume Expansion</strong></h4>\n<ul>\n<li>The underlying provider and CSI driver for the persistent volume must support volume expansion. You should refer to the documentation of your CSI driver to check the capability. (Managed Kubernetes with their inbuilt CSI driver supports this natively and the feature volume expansion is also\u00a0enabled)</li>\n<li>Volume expansion must be enabled on the respective storage class used by the persistent volume. This can be checked using the below\u00a0command:</li>\n</ul>\n<pre>## Output in Name of storageclass with support to ALLOWVOLUMEEXPANSION<br>$ kubectl get storageclass -o custom-columns=NAME:.metadata.name,ALLOWVOLUMEEXPANSION:.allowVolumeExpansion</pre>\n<p>Output from the above command will look something like\u00a0this:</p>\n<pre>NAME                    ALLOWVOLUMEEXPANSION<br>azurefile               true<br>azurefile-csi           true<br>azurefile-csi-premium   true<br>azurefile-premium       true<br>default                 true<br>managed                 true<br>managed-csi             true<br>managed-csi-premium     true<br>managed-premium         true</pre>\n<h4><strong>Step-by-Step Guide With\u00a0Example.</strong></h4>\n<p>To make it more practical, I will use the Alertmanager application, which is installed with the <a href=\"https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack\">kube-prometheus-stack</a> helm\u00a0chart.</p>\n<p><strong>Reproduce the\u00a0problem</strong></p>\n<ul><li>Installation of helm\u00a0chart.</li></ul>\n<pre>$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts<br>$ helm repo update<br>$ helm upgrade --install kube-prom-stack prometheus-community/kube-prometheus-stack --namespace kube-prom-stack --create-namespace --values custom-values.yaml<br>Release \"kube-prom-stack\" does not exist. Installing it now.<br>NAME: kube-prom-stack<br>LAST DEPLOYED: Thu Nov 23 10:42:39 2023<br>NAMESPACE: kube-prom-stack<br>STATUS: deployed<br>REVISION: 1<br>NOTES:<br>kube-prometheus-stack has been installed. Check its status by running:<br>  kubectl --namespace kube-prom-stack get pods -l \"release=kube-prom-stack\"<br><br>Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create &amp; configure Alertmanager and Prometheus instances using the Operator.<br></pre>\n<blockquote>\n<strong>Info:</strong> Application details are out of the scope of this\u00a0article.</blockquote>\n<ul><li>custom-values.yaml</li></ul>\n<pre>alertmanager:<br>  ingress:<br>    enabled: false<br><br>  podDisruptionBudget:<br>    enabled: false<br><br>  alertmanagerSpec:<br>    storage:<br>      volumeClaimTemplate:<br>        spec:<br>          accessModes: [\"ReadWriteOnce\"]<br>          resources:<br>            requests:<br>              storage: 2Gi</pre>\n<p>Once the installation is complete, we can list the statefulsets.</p>\n<blockquote>\n<strong>Info:</strong> kube-prom-stack namespace is set as the default namespace.</blockquote>\n<pre>$ kubectl get statefulsets<br>NAME                                                   READY   AGE<br>alertmanager-kube-prom-stack-kube-prome-alertmanager   1/1     11m<br>prometheus-kube-prom-stack-kube-prome-prometheus       1/1     11m</pre>\n<ul><li>Check the details of the Persistent volume claim created by the volumeClaimTemplates specification of the statefulset to verify if the correct values are\u00a0applied.</li></ul>\n<pre>## Get the pvc name from the statefulset volumeClaimTemplate<br>$ kubectl get sts alertmanager-kube-prom-stack-kube-prome-alertmanager -o jsonpath='{.spec.volumeClaimTemplates[0].metadata.name}' ; echo \"\"<br>alertmanager-kube-prom-stack-kube-prome-alertmanager-db<br><br>## Get the pvc size request from the statefulset volumeClaimTemplate<br>$ kubectl get sts alertmanager-kube-prom-stack-kube-prome-alertmanager -o jsonpath='{.spec.volumeClaimTemplates[0].spec.resources}' | jq<br>{<br>  \"requests\": {<br>    \"storage\": \"2Gi\"<br>  }<br>}<br><br>## Verify the size of the pvc got from the statefulset volumeClaimTemplate<br>$ kubectl get pvc alertmanager-kube-prom-stack-kube-prome-alertmanager-db-alertmanager-kube-prom-stack-kube-prome-alertmanager-0 -o jsonpath='{.spec.resources.requests}' | jq<br>{<br>  \"storage\": \"2Gi\"<br>}</pre>\n<p>Storage is 2Gi which seems to be as per our provided custom-values.yaml \u2705</p>\n<ul>\n<li>Extend the persistent volume claim storage request viacustom-values.yaml.</li>\n<li>Modified custom-values.yaml</li>\n</ul>\n<pre>alertmanager:<br>  ingress:<br>    enabled: false<br><br>  podDisruptionBudget:<br>    enabled: false<br><br>  alertmanagerSpec:<br>    storage:<br>      volumeClaimTemplate:<br>        spec:<br>          accessModes: [\"ReadWriteOnce\"]<br>          resources:<br>            requests:<br>              storage: 5Gi ## Increased ##</pre>\n<p>The size is updated to 5Gi\u00a0, will apply the new values and monitor the\u00a0changes.</p>\n<ul><li>Helm upgrade with new\u00a0values.</li></ul>\n<pre>$ helm upgrade --install kube-prom-stack prometheus-community/kube-prometheus-stack --namespace kube-prom-stack --values custom-values.yaml<br>Release \"kube-prom-stack\" has been upgraded. Happy Helming!<br>NAME: kube-prom-stack<br>LAST DEPLOYED: Thu Nov 23 10:55:25 2023<br>NAMESPACE: kube-prom-stack<br>STATUS: deployed<br>REVISION: 2<br>NOTES:<br>kube-prometheus-stack has been installed. Check its status by running:<br>  kubectl --namespace kube-prom-stack get pods -l \"release=kube-prom-stack\"<br><br>Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create &amp; configure Alertmanager and Prometheus instances using the Operator.</pre>\n<ul><li>Check the details of the Persistent volume claim after updating the\u00a0size.</li></ul>\n<pre>## Get the pvc name from the statefulset volumeClaimTemplate<br>$ kubectl get sts alertmanager-kube-prom-stack-kube-prome-alertmanager -o jsonpath='{.spec.volumeClaimTemplates[0].metadata.name}' ; echo \"\"<br>alertmanager-kube-prom-stack-kube-prome-alertmanager-db<br><br>## Get the pvc size request from the statefulset volumeClaimTemplate (UPDATED)<br>$ kubectl get sts alertmanager-kube-prom-stack-kube-prome-alertmanager -o jsonpath='{.spec.volumeClaimTemplates[0].spec.resources}' | jq<br>{<br>  \"requests\": {<br>    \"storage\": \"5Gi\"<br>  }<br>}<br><br>## Verify the size of the pvc got from the statefulset volumeClaimTemplate (NOT-UPDATED)<br>$ kubectl get pvc alertmanager-kube-prom-stack-kube-prome-alertmanager-db-alertmanager-kube-prom-stack-kube-prome-alertmanager-0 -o jsonpath='{.spec.resources.requests}' | jq<br>{<br>  \"storage\": \"2Gi\"<br>}</pre>\n<p>Even though the volumeClaimTemplate of statefulset(alertmanager) is updated the persistent volume claim is using the old size 2Gi.\u00a0\u274c</p>\n<h4>Solution</h4>\n<p>The following steps are required to increase the size from 2Gito\u00a05Gi</p>\n<ul><li>\n<strong>STEP 1:</strong> In this case, the statefulset is managed via the operator and hence needs to be paused as well on the custom resource alertmanager</li></ul>\n<pre>## Get the name of alertmanager resource<br>$ kubectl get alertmanager<br>NAME                                      VERSION   REPLICAS   READY   RECONCILED   AVAILABLE   AGE<br>kube-prom-stack-kube-prome-alertmanager   v0.26.0   1          1       True         True        61m</pre>\n<pre>## Patch the resource , so that operator does not recreates it.<br>$ kubectl patch alertmanager/kube-prom-stack-kube-prome-alertmanager  --patch '{\"spec\": {\"paused\": true, \"storage\": {\"volumeClaimTemplate\": {\"spec\": {\"resources\": {\"requests\": {\"storage\":\"5Gi\"}}}}}}}' --type merge<br>alertmanager.monitoring.coreos.com/kube-prom-stack-kube-prome-alertmanager patched</pre>\n<ul><li>\n<strong>STEP 2:</strong> Patch the persistent volume claims associated with the statefulset.</li></ul>\n<pre>for p in $(kubectl get pvc -l alertmanager=kube-prom-stack-kube-prome-alertmanager -o jsonpath='{range .items[*]}{.metadata.name} {end}'); do \\<br>  kubectl patch pvc/${p} --patch '{\"spec\": {\"resources\": {\"requests\": {\"storage\":\"5Gi\"}}}}'; \\<br>done</pre>\n<ul><li>\n<strong>STEP 3:</strong> The final step is to delete the statefulset with the orphan strategy.</li></ul>\n<pre>$ kubectl delete sts alertmanager-kube-prom-stack-kube-prome-alertmanager --cascade=orphan</pre>\n<p><em>To verify if the PVC is extended.</em></p>\n<pre>## PVC is updated<br>$ kubectl get pvc alertmanager-kube-prom-stack-kube-prome-alertmanager-db-alertmanager-kube-prom-stack-kube-prome-alertmanager-0 -o jsonpath='{.spec.resources.requests}' | jq<br>{<br>  \"storage\": \"5Gi\"<br>}</pre>\n<p>By passing --cascade=orphan to kubectl delete, the Pods managed by the StatefulSet are left behind even after the StatefulSet object itself is\u00a0deleted.</p>\n<ul><li>\n<strong>STEP 4:</strong> Resume the operator to take control of the alertmanager provisioning. This step is valid in statefulsets managed via operators only.</li></ul>\n<pre>$ kubectl patch alertmanager/kube-prom-stack-kube-prome-alertmanager --patch '{\"spec\": {\"paused\": false}}' --type merge</pre>\n<h3>Conclusion</h3>\n<p>In Summary following steps need to be followed in the respective order to expand the persistent volume on statefulsets.</p>\n<ol>\n<li>Patch the statefulset with the required(expanded) volumeClaimTemplates configuration.</li>\n<li>Patch the persistent volume claims with the required(expanded) size configuration associated with the statefulset.</li>\n<li>Delete the statefulset with cascade=orphan strategy.</li>\n</ol>\n<p>And that concludes our exploration into the dynamic world of scaling storage for stateful applications in Kubernetes.</p>\n<p>Expanding the persistent volumes in statefulsets is a well-known restriction and is been tracked in the <a href=\"https://github.com/kubernetes/enhancements/issues/661\">Support Volume Expansion Through StatefulSets</a> and <a href=\"https://github.com/kubernetes/kubernetes/issues/68737\">StatefulSet: support resize pvc storage in K8s v1.11</a> GitHub issues in official Kubernetes repositories.</p>\n<h3>Acknowledgments</h3>\n<p>Thank you for embarking on this Kubernetes adventure with me. Scaling storage in Kubernetes can be complex, but you\u2019ve gained valuable insights to navigate this terrain. May your clusters be resilient, and your applications thrive. Happy reading, and until our next exploration!</p>\n<h3>Helpful Links</h3>\n<ul>\n<li><a href=\"https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/\">Kubernetes 1.24: Volume Expansion Now A Stable\u00a0Feature</a></li>\n<li><a href=\"https://prometheus-operator.dev/docs/operator/storage/#resizing-volumes\">Resizing Prometheus Operator\u00a0Volumes</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/68737\">StatefulSet: support resize pvc storage in K8s\u00a0v1.11</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims\">Expanding Persistent Volumes\u00a0Claims</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ecd7cf75b754\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*zTlqffP7_Fd9nHPJ\"><figcaption>Photo by <a href=\"https://unsplash.com/@growtika?utm_source=medium&amp;utm_medium=referral\">Growtika</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3><strong>Expanding Persistent Volumes for StatefulSets in Kubernetes</strong></h3>\n<p>Expanding volumes in Kubernetes clusters is a routine maintenance activity and seemingly straightforward\u200a\u2014\u200aor so I thought! Recently, a unique challenge arose when there was a request to increase the storage size for a Stateful Application running in a Kubernetes cluster.</p>\n<p>Let\u2019s jump into the story to share the real-world experience of this\u00a0request.</p>\n<h4><strong>Problem Statement</strong></h4>\n<p>In the realm of Kubernetes clusters, a statefulset application thrived. Its popularity soared to the extent that it demanded more storage than the currently attached volume could provide. The team, recognizing the need to meet this demand, set out to expand the storage of the persistent volume connected to the statefulset.<em> However, quickly realized that this was easier said than\u00a0done.</em></p>\n<p>As<a href=\"https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/\"> Kubernetes 1.24</a> supports volume expansion, Just like anyone who had the first encounter with this task, I anticipated that it was as simple as increasing the spec.volumeClaimTemplates.spec.resources.requests.storage in the statefulset application. <strong>**But**</strong> It was not\u00a0!!</p>\n<p>Contrary to initial expectations, modifying the spec.volumeClaimTemplates.spec.resources.requests.storage alone does not trigger the expansion of volumes attached to statefulsets. Manual intervention is necessary to navigate this intricate landscape. Let\u2019s learn how can we achieve it in this\u00a0article.</p>\n<h4><strong>Prerequisites For Persistent Volume Expansion</strong></h4>\n<ul>\n<li>The underlying provider and CSI driver for the persistent volume must support volume expansion. You should refer to the documentation of your CSI driver to check the capability. (Managed Kubernetes with their inbuilt CSI driver supports this natively and the feature volume expansion is also\u00a0enabled)</li>\n<li>Volume expansion must be enabled on the respective storage class used by the persistent volume. This can be checked using the below\u00a0command:</li>\n</ul>\n<pre>## Output in Name of storageclass with support to ALLOWVOLUMEEXPANSION<br>$ kubectl get storageclass -o custom-columns=NAME:.metadata.name,ALLOWVOLUMEEXPANSION:.allowVolumeExpansion</pre>\n<p>Output from the above command will look something like\u00a0this:</p>\n<pre>NAME                    ALLOWVOLUMEEXPANSION<br>azurefile               true<br>azurefile-csi           true<br>azurefile-csi-premium   true<br>azurefile-premium       true<br>default                 true<br>managed                 true<br>managed-csi             true<br>managed-csi-premium     true<br>managed-premium         true</pre>\n<h4><strong>Step-by-Step Guide With\u00a0Example.</strong></h4>\n<p>To make it more practical, I will use the Alertmanager application, which is installed with the <a href=\"https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack\">kube-prometheus-stack</a> helm\u00a0chart.</p>\n<p><strong>Reproduce the\u00a0problem</strong></p>\n<ul><li>Installation of helm\u00a0chart.</li></ul>\n<pre>$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts<br>$ helm repo update<br>$ helm upgrade --install kube-prom-stack prometheus-community/kube-prometheus-stack --namespace kube-prom-stack --create-namespace --values custom-values.yaml<br>Release \"kube-prom-stack\" does not exist. Installing it now.<br>NAME: kube-prom-stack<br>LAST DEPLOYED: Thu Nov 23 10:42:39 2023<br>NAMESPACE: kube-prom-stack<br>STATUS: deployed<br>REVISION: 1<br>NOTES:<br>kube-prometheus-stack has been installed. Check its status by running:<br>  kubectl --namespace kube-prom-stack get pods -l \"release=kube-prom-stack\"<br><br>Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create &amp; configure Alertmanager and Prometheus instances using the Operator.<br></pre>\n<blockquote>\n<strong>Info:</strong> Application details are out of the scope of this\u00a0article.</blockquote>\n<ul><li>custom-values.yaml</li></ul>\n<pre>alertmanager:<br>  ingress:<br>    enabled: false<br><br>  podDisruptionBudget:<br>    enabled: false<br><br>  alertmanagerSpec:<br>    storage:<br>      volumeClaimTemplate:<br>        spec:<br>          accessModes: [\"ReadWriteOnce\"]<br>          resources:<br>            requests:<br>              storage: 2Gi</pre>\n<p>Once the installation is complete, we can list the statefulsets.</p>\n<blockquote>\n<strong>Info:</strong> kube-prom-stack namespace is set as the default namespace.</blockquote>\n<pre>$ kubectl get statefulsets<br>NAME                                                   READY   AGE<br>alertmanager-kube-prom-stack-kube-prome-alertmanager   1/1     11m<br>prometheus-kube-prom-stack-kube-prome-prometheus       1/1     11m</pre>\n<ul><li>Check the details of the Persistent volume claim created by the volumeClaimTemplates specification of the statefulset to verify if the correct values are\u00a0applied.</li></ul>\n<pre>## Get the pvc name from the statefulset volumeClaimTemplate<br>$ kubectl get sts alertmanager-kube-prom-stack-kube-prome-alertmanager -o jsonpath='{.spec.volumeClaimTemplates[0].metadata.name}' ; echo \"\"<br>alertmanager-kube-prom-stack-kube-prome-alertmanager-db<br><br>## Get the pvc size request from the statefulset volumeClaimTemplate<br>$ kubectl get sts alertmanager-kube-prom-stack-kube-prome-alertmanager -o jsonpath='{.spec.volumeClaimTemplates[0].spec.resources}' | jq<br>{<br>  \"requests\": {<br>    \"storage\": \"2Gi\"<br>  }<br>}<br><br>## Verify the size of the pvc got from the statefulset volumeClaimTemplate<br>$ kubectl get pvc alertmanager-kube-prom-stack-kube-prome-alertmanager-db-alertmanager-kube-prom-stack-kube-prome-alertmanager-0 -o jsonpath='{.spec.resources.requests}' | jq<br>{<br>  \"storage\": \"2Gi\"<br>}</pre>\n<p>Storage is 2Gi which seems to be as per our provided custom-values.yaml \u2705</p>\n<ul>\n<li>Extend the persistent volume claim storage request viacustom-values.yaml.</li>\n<li>Modified custom-values.yaml</li>\n</ul>\n<pre>alertmanager:<br>  ingress:<br>    enabled: false<br><br>  podDisruptionBudget:<br>    enabled: false<br><br>  alertmanagerSpec:<br>    storage:<br>      volumeClaimTemplate:<br>        spec:<br>          accessModes: [\"ReadWriteOnce\"]<br>          resources:<br>            requests:<br>              storage: 5Gi ## Increased ##</pre>\n<p>The size is updated to 5Gi\u00a0, will apply the new values and monitor the\u00a0changes.</p>\n<ul><li>Helm upgrade with new\u00a0values.</li></ul>\n<pre>$ helm upgrade --install kube-prom-stack prometheus-community/kube-prometheus-stack --namespace kube-prom-stack --values custom-values.yaml<br>Release \"kube-prom-stack\" has been upgraded. Happy Helming!<br>NAME: kube-prom-stack<br>LAST DEPLOYED: Thu Nov 23 10:55:25 2023<br>NAMESPACE: kube-prom-stack<br>STATUS: deployed<br>REVISION: 2<br>NOTES:<br>kube-prometheus-stack has been installed. Check its status by running:<br>  kubectl --namespace kube-prom-stack get pods -l \"release=kube-prom-stack\"<br><br>Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create &amp; configure Alertmanager and Prometheus instances using the Operator.</pre>\n<ul><li>Check the details of the Persistent volume claim after updating the\u00a0size.</li></ul>\n<pre>## Get the pvc name from the statefulset volumeClaimTemplate<br>$ kubectl get sts alertmanager-kube-prom-stack-kube-prome-alertmanager -o jsonpath='{.spec.volumeClaimTemplates[0].metadata.name}' ; echo \"\"<br>alertmanager-kube-prom-stack-kube-prome-alertmanager-db<br><br>## Get the pvc size request from the statefulset volumeClaimTemplate (UPDATED)<br>$ kubectl get sts alertmanager-kube-prom-stack-kube-prome-alertmanager -o jsonpath='{.spec.volumeClaimTemplates[0].spec.resources}' | jq<br>{<br>  \"requests\": {<br>    \"storage\": \"5Gi\"<br>  }<br>}<br><br>## Verify the size of the pvc got from the statefulset volumeClaimTemplate (NOT-UPDATED)<br>$ kubectl get pvc alertmanager-kube-prom-stack-kube-prome-alertmanager-db-alertmanager-kube-prom-stack-kube-prome-alertmanager-0 -o jsonpath='{.spec.resources.requests}' | jq<br>{<br>  \"storage\": \"2Gi\"<br>}</pre>\n<p>Even though the volumeClaimTemplate of statefulset(alertmanager) is updated the persistent volume claim is using the old size 2Gi.\u00a0\u274c</p>\n<h4>Solution</h4>\n<p>The following steps are required to increase the size from 2Gito\u00a05Gi</p>\n<ul><li>\n<strong>STEP 1:</strong> In this case, the statefulset is managed via the operator and hence needs to be paused as well on the custom resource alertmanager</li></ul>\n<pre>## Get the name of alertmanager resource<br>$ kubectl get alertmanager<br>NAME                                      VERSION   REPLICAS   READY   RECONCILED   AVAILABLE   AGE<br>kube-prom-stack-kube-prome-alertmanager   v0.26.0   1          1       True         True        61m</pre>\n<pre>## Patch the resource , so that operator does not recreates it.<br>$ kubectl patch alertmanager/kube-prom-stack-kube-prome-alertmanager  --patch '{\"spec\": {\"paused\": true, \"storage\": {\"volumeClaimTemplate\": {\"spec\": {\"resources\": {\"requests\": {\"storage\":\"5Gi\"}}}}}}}' --type merge<br>alertmanager.monitoring.coreos.com/kube-prom-stack-kube-prome-alertmanager patched</pre>\n<ul><li>\n<strong>STEP 2:</strong> Patch the persistent volume claims associated with the statefulset.</li></ul>\n<pre>for p in $(kubectl get pvc -l alertmanager=kube-prom-stack-kube-prome-alertmanager -o jsonpath='{range .items[*]}{.metadata.name} {end}'); do \\<br>  kubectl patch pvc/${p} --patch '{\"spec\": {\"resources\": {\"requests\": {\"storage\":\"5Gi\"}}}}'; \\<br>done</pre>\n<ul><li>\n<strong>STEP 3:</strong> The final step is to delete the statefulset with the orphan strategy.</li></ul>\n<pre>$ kubectl delete sts alertmanager-kube-prom-stack-kube-prome-alertmanager --cascade=orphan</pre>\n<p><em>To verify if the PVC is extended.</em></p>\n<pre>## PVC is updated<br>$ kubectl get pvc alertmanager-kube-prom-stack-kube-prome-alertmanager-db-alertmanager-kube-prom-stack-kube-prome-alertmanager-0 -o jsonpath='{.spec.resources.requests}' | jq<br>{<br>  \"storage\": \"5Gi\"<br>}</pre>\n<p>By passing --cascade=orphan to kubectl delete, the Pods managed by the StatefulSet are left behind even after the StatefulSet object itself is\u00a0deleted.</p>\n<ul><li>\n<strong>STEP 4:</strong> Resume the operator to take control of the alertmanager provisioning. This step is valid in statefulsets managed via operators only.</li></ul>\n<pre>$ kubectl patch alertmanager/kube-prom-stack-kube-prome-alertmanager --patch '{\"spec\": {\"paused\": false}}' --type merge</pre>\n<h3>Conclusion</h3>\n<p>In Summary following steps need to be followed in the respective order to expand the persistent volume on statefulsets.</p>\n<ol>\n<li>Patch the statefulset with the required(expanded) volumeClaimTemplates configuration.</li>\n<li>Patch the persistent volume claims with the required(expanded) size configuration associated with the statefulset.</li>\n<li>Delete the statefulset with cascade=orphan strategy.</li>\n</ol>\n<p>And that concludes our exploration into the dynamic world of scaling storage for stateful applications in Kubernetes.</p>\n<p>Expanding the persistent volumes in statefulsets is a well-known restriction and is been tracked in the <a href=\"https://github.com/kubernetes/enhancements/issues/661\">Support Volume Expansion Through StatefulSets</a> and <a href=\"https://github.com/kubernetes/kubernetes/issues/68737\">StatefulSet: support resize pvc storage in K8s v1.11</a> GitHub issues in official Kubernetes repositories.</p>\n<h3>Acknowledgments</h3>\n<p>Thank you for embarking on this Kubernetes adventure with me. Scaling storage in Kubernetes can be complex, but you\u2019ve gained valuable insights to navigate this terrain. May your clusters be resilient, and your applications thrive. Happy reading, and until our next exploration!</p>\n<h3>Helpful Links</h3>\n<ul>\n<li><a href=\"https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/\">Kubernetes 1.24: Volume Expansion Now A Stable\u00a0Feature</a></li>\n<li><a href=\"https://prometheus-operator.dev/docs/operator/storage/#resizing-volumes\">Resizing Prometheus Operator\u00a0Volumes</a></li>\n<li><a href=\"https://github.com/kubernetes/kubernetes/issues/68737\">StatefulSet: support resize pvc storage in K8s\u00a0v1.11</a></li>\n<li><a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims\">Expanding Persistent Volumes\u00a0Claims</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ecd7cf75b754\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["technology","persistent-volume","statefulsets","kubernetes","devops"]}]}